{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f620d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede18344",
   "metadata": {},
   "source": [
    "Data Extraction from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4859413",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = 'https://rickandmortyapi.com/api/'\n",
    "endPoint1 = 'character'\n",
    "endPoint2 = 'location'\n",
    "endPoint3 = 'episode'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a786e0c",
   "metadata": {},
   "source": [
    "Extracting Data from 3 different endpoints and storing it as json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9382196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save(base_url, filename, total_pages=42): # Function to extract data and save as JSON\n",
    "    all_data = [] # List to hold all data\n",
    "\n",
    "    for page in range(1, total_pages + 1): # Loop through pages\n",
    "        url = f\"{base_url}?page={page}\" # Construct URL for each page\n",
    "        response = requests.get(url) # Make GET request\n",
    "\n",
    "        if response.status_code == 200: # Check if request was successful\n",
    "            data = response.json() # Parse JSON response\n",
    "            all_data.append(data) # Append data to list\n",
    "            print(f\"Page {page} extracted\") # Log progress\n",
    "        else:\n",
    "            print(f\"Failed at page {page}\") # Log failure\n",
    "            break\n",
    "\n",
    "    os.makedirs(\"raw_data\", exist_ok=True) # Ensure directory exists\n",
    "    with open(f\"raw_data/{filename}.json\", \"w\") as f: # Save data to JSON file\n",
    "        json.dump(all_data, f, indent=4) # Dump JSON data to file\n",
    "\n",
    "    return all_data # Return the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d46b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 extracted\n",
      "Page 2 extracted\n",
      "Page 3 extracted\n",
      "Page 4 extracted\n",
      "Page 5 extracted\n",
      "Page 6 extracted\n",
      "Page 7 extracted\n",
      "Page 8 extracted\n",
      "Page 9 extracted\n",
      "Page 10 extracted\n",
      "Page 11 extracted\n",
      "Page 12 extracted\n",
      "Page 13 extracted\n",
      "Page 14 extracted\n",
      "Page 15 extracted\n",
      "Page 16 extracted\n",
      "Page 17 extracted\n",
      "Page 18 extracted\n",
      "Page 19 extracted\n",
      "Page 20 extracted\n",
      "Page 21 extracted\n",
      "Page 22 extracted\n",
      "Page 23 extracted\n",
      "Page 24 extracted\n",
      "Page 25 extracted\n",
      "Page 26 extracted\n",
      "Page 27 extracted\n",
      "Page 28 extracted\n",
      "Page 29 extracted\n",
      "Page 30 extracted\n",
      "Page 31 extracted\n",
      "Page 32 extracted\n",
      "Page 33 extracted\n",
      "Page 34 extracted\n",
      "Page 35 extracted\n",
      "Page 36 extracted\n",
      "Page 37 extracted\n",
      "Page 38 extracted\n",
      "Page 39 extracted\n",
      "Page 40 extracted\n",
      "Page 41 extracted\n",
      "Page 42 extracted\n",
      "Page 1 extracted\n",
      "Page 2 extracted\n",
      "Page 3 extracted\n",
      "Page 4 extracted\n",
      "Page 5 extracted\n",
      "Page 6 extracted\n",
      "Page 7 extracted\n",
      "Failed at page 8\n",
      "Page 1 extracted\n",
      "Page 2 extracted\n",
      "Page 3 extracted\n",
      "Failed at page 4\n"
     ]
    }
   ],
   "source": [
    "# Fetch Initial Data\n",
    "initial_data1 = extract_and_save(baseURL + endPoint1, \"raw_data_characters\") # Fetch and save character data\n",
    "initial_data2 = extract_and_save(baseURL + endPoint2, \"raw_data_episodes\") # Fetch and save episode data\n",
    "initial_data3 = extract_and_save(baseURL + endPoint3, \"raw_data_locations\") # Fetch and save location data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9509d",
   "metadata": {},
   "source": [
    "Failed at page 8 because episodes only have 7 pages,\n",
    "Failed at page 4 because locations only have 3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw_data/raw_data_characters.json\", \"r\") as f: # Load raw character data from JSON file\n",
    "    raw_characters = json.load(f) # Load JSON data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d615808",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_list = [] # List to hold processed character data\n",
    "\n",
    "for page in raw_characters: # Iterate through each page of character data\n",
    "    for char in page[\"results\"]: # Iterate through each character in the results\n",
    "        characters_list.append({ # Append selected character attributes to the list\n",
    "            \"id\": char[\"id\"],\n",
    "            \"name\": char[\"name\"],\n",
    "            \"status\": char[\"status\"],\n",
    "            \"species\": char[\"species\"],\n",
    "            \"type\": char[\"type\"],\n",
    "            \"gender\": char[\"gender\"]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9870be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id          name   status species                  type      gender\n",
      "0      1  Rick Sanchez    Alive   Human                              Male\n",
      "1      2   Morty Smith    Alive   Human                              Male\n",
      "2      3  Summer Smith    Alive   Human                            Female\n",
      "3      4    Beth Smith    Alive   Human                            Female\n",
      "4      5   Jerry Smith    Alive   Human                              Male\n",
      "..   ...           ...      ...     ...                   ...         ...\n",
      "821  822   Young Jerry  unknown   Human                              Male\n",
      "822  823    Young Beth  unknown   Human                            Female\n",
      "823  824    Young Beth  unknown   Human                            Female\n",
      "824  825   Young Jerry  unknown   Human                              Male\n",
      "825  826  Butter Robot    Alive   Robot  Passing Butter Robot  Genderless\n",
      "\n",
      "[826 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_characters = pd.DataFrame(characters_list) # Create DataFrame from character list\n",
    "\n",
    "print(df_characters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
